---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

I am an upcoming joint **PhD student** at **the State Key Laboratory of General Artificial Intelligence** <a href='https://www.bigai.ai/'>(BIGAI)</a> and **the University of Science and Technology of China** (USTC), supervised by <a href='https://liqing.io/'>Qing Li(BIGAI, ÊùéÂ∫Ü)</a>, <a href='https://yuntaodu.github.io/'>Yuntao Du(SDU, Êùú‰∫ëÊ∂õ)</a>, <a href='https://siyuanqi.github.io/'>Siyuan Qi (Gyges Labs, Á∂¶ÊÄùÊ∫ê)</a> and <a href='http://staff.ustc.edu.cn/~binli/'>Bin Li (USTC, ÊùéÊñå)</a>, <a href='https://faculty.ustc.edu.cn/liulei13/zh_CN/index.htm'>Lei Liu (USTC, ÂàòÁ£ä)</a>. Currently, I am doing my internship in State Key Laboratory of General Artificial Intelligence.


I am currently working on knowledge editing, knowledge injection, multimodal learning, continual learning, <a href='https://scholar.google.com/citations?user=NSHQsrAAAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FKailinJiang%2Fkailinjiang.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations">


My huggingface at ü§ó [Huggingface home](https://huggingface.co/kailinjiang).

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->



# üî• News
<!-- Allowed emojis: üéâüéâfor good news üì£üì£for average news-->
- **2025.03**: &nbsp;üéâüéâ One paper have been accepted by **ICLR 2025 Workshop SSI-FM**! <a href='https://mmke-bench-iclr.github.io/'>Multimodal Knowledge Editing</a>ÔºÅ
- **2025.01**: &nbsp;üéâüéâ Two paper have been accepted by **ICLR 2025**! <a href='https://arxiv.org/pdf/2406.11194'>In Context Editing</a> and <a href='https://mmke-bench-iclr.github.io/'>Multimodal Knowledge Editing</a>ÔºÅ
- **2024.06**: &nbsp;üéâüéâ I successfully completed my undergraduate studies from the College of Science of Sichuan Agricultural University!
- **2024.02**: &nbsp;üì£üì£ I will go to the State Key Laboratory of General Artificial Intelligence <a href='https://www.bigai.ai/'>(BIGAI)</a> to start my internship!


# üìù Publications
*: Co-First Author, Equal Contribution


<div style="display: flex; justify-content: center; gap: 20px;">
  <div style="display: flex; align-items: center;">
    <img src="./images/conference/ICLR.png" alt="ICLR" style="width: 150px; height: 45px;">
    <span style="margin-left: 10px; font-size: 20px; font-weight: bold; color: #8B4513;">√ó2</span>
  </div>
  <div style="display: flex; align-items: center;">
    <img src="./images/conference/blank.png" alt="ICLR" style="width: 150px; height: 45px;">
    <span style="margin-left: 10px; font-size: 20px; font-weight: bold; color: #8B4513;"></span>
  </div>
  <div style="display: flex; align-items: center;">
    <img src="./images/conference/blank.png" alt="ICLR" style="width: 150px; height: 45px;">
    <span style="margin-left: 10px; font-size: 20px; font-weight: bold; color: #8B4513;"></span>
  </div>
  <div style="display: flex; align-items: center;">
    <img src="./images/conference/blank.png" alt="ICLR" style="width: 150px; height: 45px;">
    <span style="margin-left: 10px; font-size: 20px; font-weight: bold; color: #8B4513;"></span>
  </div>
  <div style="display: flex; align-items: center;">
    <img src="./images/conference/blank.png" alt="ICLR" style="width: 150px; height: 45px;">
    <span style="margin-left: 10px; font-size: 20px; font-weight: bold; color: #8B4513;"></span>
  </div>
</div>





<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR2025 & ICLR2025 Workshop SSI-FM</div><img src='images/paper_overview/MMKE-Bench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge](https://arxiv.org/pdf/2502.19870)

 Yuntao Du\*,**Kailin Jiang\***, Zhi Gao, Chenrui Shi, Zilong Zheng, Siyuan Qi, Qing Li. „Äê2024.10„Äë <br>
 <b style="color: #8B0000;">The Thirteenth International Conference on Learning Representations</b>
[![arXiv](https://img.shields.io/badge/Arxiv-2502.19870-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2502.19870)  [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-MMKE_Bench-blue)](https://huggingface.co/datasets/kailinjiang/MMKE-Bench-dataset)  [![paperwithcode](https://img.shields.io/badge/PWC-MMKE_Bench-blue?logo=paperswithcode)](https://paperswithcode.com/paper/mmke-bench-a-multimodal-editing-benchmark-for) [![code](https://img.shields.io/badge/Code-MMKE_Bench-blue?logo=github)](https://github.com/MMKE-Bench-ICLR/MMKE-Bench) [![website](https://img.shields.io/badge/Website-MMKE_Bench-orange?logo=homepage)](https://mmke-bench-iclr.github.io/) [![depositphotos](https://img.shields.io/badge/Poster-MMKE_Bench-red?logo=depositphotos)](/images/poster/iclr25_mmke_bench_poster.pdf)

[![airchina](https://img.shields.io/badge/Êï∞Ê∫êAI-MMKE_Bench-red?logo=airchina)](https://mp.weixin.qq.com/s/iN826lITi5Xyz-3GnrdVIQ) [![codeproject](https://img.shields.io/badge/ÈáèÂ≠ê‰πãÂøÉ-MMKE_Bench-red?logo=codeproject)](https://www.xiaohongshu.com/explore/67e2d622000000000603cbfc?note_flow_source=wechat&xsec_token=CBldN8wUavDAzFvP4tK_noXO94RAXcelKKqlO3pFiJ6EQ=) [![actix](https://img.shields.io/badge/ÊûÅÂ∏ÇÂπ≥Âè∞-MMKE_Bench-red?logo=actix)](https://mp.weixin.qq.com/s/JfxeytzWU0QoIUfJTGqgQQ) [![zhihu](https://img.shields.io/badge/Áü•‰πé-MMKE_Bench-red?logo=zhihu)](https://zhuanlan.zhihu.com/p/30599722521) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR2025</div><img src='images/paper_overview/in-context-editing.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[In-Context Editing:Learning Knowledge from Self-Induced Distributions](https://arxiv.org/pdf/2406.11194)

Siyuan Qi\*, Bangcheng Yang\*, **Kailin Jiang\***, Xiaobo Wang, Jiaqi Li, Yifan Zhong, Yaodong Yang, Zilong Zheng. „Äê2024.06„Äë <br>
<b style="color: #8B0000;">The Thirteenth International Conference on Learning Representations</b>
[![arXiv](https://img.shields.io/badge/Arxiv-2406.11194-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2406.11194)  [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-ICE-blue)](https://huggingface.co/datasets/Yofuria/ICE)  [![paperwithcode](https://img.shields.io/badge/PWC-ICE-blue?logo=paperswithcode)](https://paperswithcode.com/paper/in-context-editing-learning-knowledge-from) [![code](https://img.shields.io/badge/Code-ICE-blue?logo=github)](https://github.com/bigai-ai/ICE) [![depositphotos](https://img.shields.io/badge/Poster-ICE-red?logo=depositphotos)](/images/poster/ICE_poster.png)


[![AIModels.fyi](https://img.shields.io/badge/AIModels.fyi-ICE-blue?logo=anthropic)](https://www.aimodels.fyi/papers/arxiv/context-editing-learning-knowledge-from-self-induced) [![actix](https://img.shields.io/badge/ÊûÅÂ∏ÇÂπ≥Âè∞-ICE-red?logo=actix)](https://mp.weixin.qq.com/s/Mr9HPeHJSsVfUIeF6j-zWw)

</div>
</div>



# üìù Preprints
*: Co-First Author, Equal Contribution

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/paper_overview/EVOKE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[When Large Multimodal Models Confront Evolving Knowledge: Challenges and Pathways](https://arxiv.org/abs/2505.24449)

 **Kailin Jiang\***, Yuntao Du\*, Yukai Ding, Yuchen Ren, Ning Jiang, Zhi Gao, Zilong Zheng, Lei Liu, Bin Li, Qing Li.„Äê2025.6„Äë<br>
[![arXiv](https://img.shields.io/badge/Arxiv-2505.24449-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2505.24449) [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-EVOKE-blue)](https://huggingface.co/datasets/kailinjiang/EVOKE)   [![code](https://img.shields.io/badge/Code-EVOKE-blue?logo=github)](https://github.com/EVOKE-LMM/EVOKE) [![website](https://img.shields.io/badge/Website-EVOKE-orange?logo=homepage)](https://evoke-lmm.github.io/)
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/paper_overview/MMKC.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models](https://arxiv.org/pdf/2505.19509)

 Yifan Jia\*, **Kailin Jiang\***, Yuyang Liang, Qihan Ren, Yi Xin, Rui Yang, Fenze Feng, Mingcai Chen, Hengyang Lu, Haozhe Wang, Xiaoye Qu, Dongrui Liu, Lizhen Cui, Yuntao Du. „Äê2025.5„Äë<br>
[![arXiv](https://img.shields.io/badge/Arxiv-2505.19509-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2505.19509) [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-MMKC_Bench-blue)](https://huggingface.co/datasets/starjyf/MLLMKC-datasets)   [![code](https://img.shields.io/badge/Code-MMKC_Bench-blue?logo=github)](https://github.com/MLLMKCBENCH/MLLMKC) [![website](https://img.shields.io/badge/Website-MMKC_Bench-orange?logo=homepage)](https://mllmkcbench.github.io/)
</div>
</div>




# üì∞ Peer Review
- ICLR 2025 Workshop SSI-FM Reviewer


# üéñ Honors and Awards
- **2022.11** China Telecom Scholarship ¬∑ Fly Young Award.
  
- **2021.11** National First Prize of Undergraduate Group of National Undergraduate Mathematical Modeling Contest of Gaojiaoshe Cup,team leader. 


# üìñ Educations
- **2024.06 - now**, <img src='./images/ustc.png' style='width: 2em;'> **University of Science and Technology of China (USTC), PhD student**. I am pursuing a degree in Information and Communication Engineering at USTC's School of Information Science and Technology, and the program is co-training with the State Key Laboratory of General Artificial Intelligence.

- **2020.09 - 2024.06**, **Sichuan Agricultural University (SICAU), graduate student**. I am studying for a degree in Information and Computational Science at the college of science in SICAU.


# üíª Internships
- **2024.08 - now**, <img src='./images/internship/bigai.png' style='width: 6em;'> the State Key Laboratory of General Artificial Intelligence(Beijing,China), **ML Lab**, Intern Researcher.
- **2024.02 - 2024.08**, <img src='./images/internship/bigai.png' style='width: 6em;'> the State Key Laboratory of General Artificial Intelligence(Beijing,China), **MAS Lab**, Algorithm Intern.



<div style="display: flex; justify-content: center;">
  <div style="text-align: center;">
    <img src="./images/internship/bigai.png" alt="BIGAI" style="width: 200px; height: 60px;">
  </div>
</div>






<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Map Widget</title>
    <style>
        .map-container {
            width: 300px; 
            margin: 0 auto; 
            text-align: center; 
        }
        .map-container iframe {
            width: 100%; 
            height: 300px; 
        }
    </style>
</head>
<body>
    <div class="map-container">
<script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=je5F_lWiCT9d_1nlJrSDAQ7oBIhafAq9Er1Q9DPpYXg&cl=ffffff&w=a"></script>
    </div>
</body>
</html>
